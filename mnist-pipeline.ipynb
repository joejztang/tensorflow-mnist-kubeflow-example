{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fed67e35-d0ca-4577-b94f-892763b1bf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "from kfp.dsl import Input, Output\n",
    "from kfp.dsl import Dataset, Artifact\n",
    "from kfp.dsl import Model, Metrics, ClassificationMetrics\n",
    "\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "46075f38-7921-433b-bbc1-970c78d65cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "BASE_IMAGE = \"tensorflow/tensorflow\"\n",
    "\n",
    "devices = tf.config.list_physical_devices('GPU')\n",
    "if devices:\n",
    "    BASE_IMAGE += \":latest-gpu\"\n",
    "else:\n",
    "    BASE_IMAGE += \":latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "351c0661-8ae3-4c78-b840-d16bc5a915a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTICE: cannot declare outside of component\n",
    "# class MyCallback(tf.keras.callbacks.Callback):\n",
    "#   def on_epoch_end(self, epoch, logs={}):\n",
    "#     if(logs.get('accuracy')>0.995):\n",
    "#       print(\"\\nReached 99.5% accuracy so cancelling training!\")\n",
    "#       self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "638d991f-e257-4dcd-99de-aa28ed839448",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=BASE_IMAGE,\n",
    ")\n",
    "def load_data(\n",
    "    x_train_input: Output[Dataset],\n",
    "    y_train_input: Output[Dataset],\n",
    "    x_test_input: Output[Dataset],\n",
    "    y_test_input: Output[Dataset],\n",
    "):\n",
    "    from keras.datasets import mnist\n",
    "    import pickle\n",
    "\n",
    "    # load dataset\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "    with open(x_train_input.path, \"wb\") as file:\n",
    "        pickle.dump(x_train, file)\n",
    "\n",
    "    with open(y_train_input.path, \"wb\") as file:\n",
    "        pickle.dump(y_train, file)\n",
    "\n",
    "    with open(x_test_input.path, \"wb\") as file:\n",
    "        pickle.dump(x_test, file)\n",
    "\n",
    "    with open(y_test_input.path, \"wb\") as file:\n",
    "        pickle.dump(y_test, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "562dd1c2-84b3-46ad-89aa-ad9b1f4572db",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image = BASE_IMAGE,\n",
    ")\n",
    "def preprocess_data(\n",
    "    x_train_input: Input[Dataset],\n",
    "    y_train_input: Input[Dataset],\n",
    "    x_test_input: Input[Dataset],\n",
    "    y_test_input: Input[Dataset],\n",
    "    x_train_pre: Output[Dataset],\n",
    "    y_train_pre: Output[Dataset],\n",
    "    x_test_pre: Output[Dataset],\n",
    "    y_test_pre: Output[Dataset],\n",
    ") -> NamedTuple(\"Preprocessed\", input_shape=str, unique_classes=int):\n",
    "    import pickle\n",
    "    import numpy as np\n",
    "\n",
    "    \n",
    "    # load data from last step\n",
    "    with open(x_train_input.path, \"rb\") as file:\n",
    "        x_train = pickle.load(file)\n",
    "    with open(y_train_input.path, \"rb\") as file:\n",
    "        y_train = pickle.load(file)\n",
    "    with open(x_test_input.path, \"rb\") as file:\n",
    "        x_test = pickle.load(file)\n",
    "    with open(y_test_input.path, \"rb\") as file:\n",
    "        y_test = pickle.load(file)\n",
    "\n",
    "    x_train=x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
    "    x_train=x_train / 255.0\n",
    "    x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)\n",
    "    x_test=x_test/255.0\n",
    "\n",
    "    with open(x_train_pre.path, \"wb\") as file:\n",
    "        pickle.dump(x_train, file)\n",
    "    with open(y_train_pre.path, \"wb\") as file:\n",
    "        pickle.dump(y_train, file)\n",
    "    with open(x_test_pre.path, \"wb\") as file:\n",
    "        pickle.dump(x_test, file)\n",
    "    with open(y_test_pre.path, \"wb\") as file:\n",
    "        pickle.dump(y_test, file)\n",
    "\n",
    "    outputs = NamedTuple(\"Preprocessed\", input_shape=str, unique_classes=int)\n",
    "    return outputs(str(x_train.shape), len(np.unique(y_train)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d38106f1-24fd-4e6e-bc4a-c65a38ab1796",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image = BASE_IMAGE,\n",
    ")\n",
    "def train(\n",
    "    x_train_pre: Input[Dataset],\n",
    "    y_train_pre: Input[Dataset],\n",
    "    model_artifact: Output[Model],\n",
    "    log: Output[Artifact],\n",
    "    *,\n",
    "    batch_size:int = 64,\n",
    "    epochs:int = 10,\n",
    "    model_version:str = \"1\",\n",
    "):\n",
    "    from datetime import datetime\n",
    "    import tensorflow as tf\n",
    "    import pickle\n",
    "    import os\n",
    "\n",
    "    from tensorflow.keras.optimizers import SGD\n",
    "    from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "    # cannot declare outside of component\n",
    "    class MyCallback(tf.keras.callbacks.Callback):\n",
    "      def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('accuracy')>0.995):\n",
    "          print(\"\\nReached 99.5% accuracy so cancelling training!\")\n",
    "          self.model.stop_training = True\n",
    "\n",
    "    log_dir = f\"{log.path}/logs/fit/{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    with open(x_train_pre.path, \"rb\") as file:\n",
    "        x_train = pickle.load(file)\n",
    "\n",
    "    with open(y_train_pre.path, \"rb\") as file:\n",
    "        y_train = pickle.load(file)\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(filters=10, kernel_size=5, strides=1, padding='valid', input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Activation('relu'),\n",
    "        tf.keras.layers.Conv2D(filters=20, kernel_size=5, strides=1, padding='valid'),\n",
    "        tf.keras.layers.SpatialDropout2D(0.5),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Activation('relu'),\n",
    "        tf.keras.layers.Reshape((320,)),\n",
    "        tf.keras.layers.Dense(units=50, input_shape=(320,)),\n",
    "        tf.keras.layers.Activation('relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=10, input_shape=(50,)),\n",
    "        tf.keras.layers.Activation('softmax')\n",
    "    ])\n",
    "\n",
    "    learning_rate = 0.01\n",
    "    momentum = 0.9\n",
    "    optimizer = SGD(learning_rate=learning_rate, momentum=momentum)\n",
    "    validation_split = 0.1\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=validation_split,\n",
    "                    callbacks=[MyCallback(), tensorboard_callback])\n",
    "\n",
    "    # NOTICE: keras 3 cannot create model if parent folders doesn't exist\n",
    "    os.makedirs(model_artifact.path, exist_ok=True)\n",
    "    model.save(f\"{model_artifact.path}/{model_version}_model.keras\")\n",
    "    # tf.saved_model.save(model, f\"{model_artifact.path}/{model_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "dfc29935-52f5-4b5d-b919-d170cc027ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=BASE_IMAGE,\n",
    "    packages_to_install=[\"scikit-learn\"],\n",
    ")\n",
    "def evaluate(\n",
    "    model_artifact: Input[Model],\n",
    "    metrics: Output[ClassificationMetrics],\n",
    "    scalar_metrics: Output[Metrics],\n",
    "    x_test_input: Input[Dataset],\n",
    "    y_test_input: Input[Dataset],\n",
    "    unique_labels:int,\n",
    "    model_version:str = \"1\",\n",
    "):\n",
    "    from tensorflow.keras.metrics import Precision\n",
    "    from tensorflow.keras.models import load_model\n",
    "    from tensorflow.keras.utils import to_categorical\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    import tensorflow as tf\n",
    "\n",
    "    # model = tf.keras.layers.TFSMLayer(f\"{model_artifact.path}/{model_version}\", call_endpoint=\"serving_default\")\n",
    "    model = load_model(f\"{model_artifact.path}/{model_version}_model.keras\")\n",
    "    # model = tf.saved_model.load(f\"{model_artifact.path}/{model_version}\")\n",
    "\n",
    "    batch_size = 128\n",
    "\n",
    "    with open(x_test_input.path, \"rb\") as file:\n",
    "        x_test = pickle.load(file)\n",
    "\n",
    "    with open(y_test_input.path, \"rb\") as file:\n",
    "        y_test = pickle.load(file)\n",
    "\n",
    "    # predictions = model(x_test)\n",
    "    predictions = model.predict(x_test, batch_size=batch_size)\n",
    "    predictions = (predictions >= 0.5).astype(int)\n",
    "\n",
    "    y_test = to_categorical(y_test)\n",
    "\n",
    "    metrics.log_confusion_matrix(\n",
    "        [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"],\n",
    "        confusion_matrix(\n",
    "            y_test.argmax(axis=1), predictions.argmax(axis=1)\n",
    "        ).tolist(),  # .tolist() to convert np array to list.\n",
    "    )\n",
    "    m = Precision()\n",
    "    m.update_state(y_test, predictions)\n",
    "    loss, acc = model.evaluate(x_test, y_test.argmax(axis=1), batch_size=batch_size)\n",
    "    scalar_metrics.log_metric(\"accuracy\", acc)\n",
    "    scalar_metrics.log_metric(\"loss\", loss)\n",
    "    scalar_metrics.log_metric(\"precision\", m.result().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f7ed4632-2eba-4e2c-b411-1e8e6ffa4567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/74a8a42f-a9b3-4cc4-8f93-1bd9026c1873\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/58ffc6b4-7071-46b2-8be8-1c84d8a76b38\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=58ffc6b4-7071-46b2-8be8-1c84d8a76b38)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"mnist_pipeline\",\n",
    ")\n",
    "def mnist_pipeline(epochs: int, model_version: str):\n",
    "    data = (\n",
    "        load_data()\n",
    "        .set_memory_limit(\"4G\")\n",
    "        .set_memory_request(\"4G\")\n",
    "        .set_cpu_limit(\"1\")\n",
    "        .set_cpu_request(\"1\")\n",
    "    )\n",
    "    preprocess = (\n",
    "        preprocess_data(\n",
    "            x_train_input=data.outputs[\"x_train_input\"],\n",
    "            y_train_input=data.outputs[\"y_train_input\"],\n",
    "            x_test_input=data.outputs[\"x_test_input\"],\n",
    "            y_test_input=data.outputs[\"y_test_input\"],\n",
    "        )\n",
    "        .set_memory_limit(\"4G\")\n",
    "        .set_memory_request(\"4G\")\n",
    "        .set_cpu_limit(\"1\")\n",
    "        .set_cpu_request(\"1\")\n",
    "    )\n",
    "    preprocess.after(data)\n",
    "    model = (\n",
    "        train(\n",
    "            x_train_pre=preprocess.outputs[\"x_train_pre\"],\n",
    "            y_train_pre=preprocess.outputs[\"y_train_pre\"],\n",
    "            epochs=epochs,\n",
    "        )\n",
    "        .set_memory_limit(\"6G\")\n",
    "        .set_memory_request(\"6G\")\n",
    "        .set_cpu_limit(\"1\")\n",
    "        .set_cpu_request(\"1\")\n",
    "    )\n",
    "    model.after(preprocess)\n",
    "    evaluation = (\n",
    "        evaluate(\n",
    "            model_artifact=model.outputs[\"model_artifact\"],\n",
    "            x_test_input=preprocess.outputs[\"x_test_pre\"],\n",
    "            y_test_input=preprocess.outputs[\"y_test_pre\"],\n",
    "            unique_labels=preprocess.outputs[\"unique_classes\"]\n",
    "        )\n",
    "        .set_memory_limit(\"4G\")\n",
    "        .set_memory_request(\"4G\")\n",
    "        .set_cpu_limit(\"1\")\n",
    "        .set_cpu_request(\"1\")\n",
    "    )\n",
    "    evaluation.after(model)\n",
    "\n",
    "\n",
    "client = kfp.Client()\n",
    "client.create_run_from_pipeline_func(\n",
    "    mnist_pipeline,\n",
    "    arguments={\"epochs\": 10, \"model_version\": \"1\"},\n",
    "    experiment_name=\"mnist_pipeline\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a49fb75-6cbc-491a-8054-2d285b6c335a",
   "metadata": {},
   "source": [
    "## Generate pipeline file for uploading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e9dc29c5-472a-46cb-9bb3-2ccaf2e380b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e55ab3b9-4c26-414c-b9cb-a536f3347825",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=mnist_pipeline,\n",
    "    package_path=\"mnist_pipeline.yaml\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579f8e53-5837-48dd-aaa2-2cf2082eea13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
